{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishus1/Drug-Efficiency-Prediction/blob/main/training_chemberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BylJ9s3sY09e"
      },
      "outputs": [],
      "source": [
        "# First, install deepchem\n",
        "!pip install --pre deepchem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8yRC1KGYNHu"
      },
      "outputs": [],
      "source": [
        "import deepchem\n",
        "deepchem.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6EXgix6Y3II"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOAEt4gsTZ5u"
      },
      "source": [
        "We want to install NVIDIA's Apex tool, for the training pipeline used by `simple-transformers` and Weights and Biases. This package enables us to use 16-bit training, mixed precision, and distributed training without any changes to our code. Generally GPUs are good at doing 32-bit(single precision) math, not at 16-bit(half) nor 64-bit(double precision). Therefore traditionally deep learning model trainings are done in 32-bit. By switching to 16-bit, weâ€™ll be using half the memory and theoretically less computation at the expense of the available number range and precision. However, pure 16-bit training creates a lot of problems for us (imprecise weight updates, gradient underflow and overflow). **Mixed precision training, with Apex, alleviates these problems**.\n",
        "\n",
        "We will be installing `simple-transformers`, a library which builds ontop of HuggingFace's `transformers` package specifically for fine-tuning ChemBERTa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjDBOn0Wmybe"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "!cd /content/apex\n",
        "!pip install -v --no-cache-dir /content/apex\n",
        "!pip install transformers\n",
        "!pip install simpletransformers\n",
        "!pip install wandb\n",
        "!cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1C_baibNUh"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!test -d bertviz_repo && echo \"FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo\"\n",
        "# !rm -r bertviz_repo # Uncomment if you need a clean pull from repo\n",
        "!test -d bertviz_repo || git clone https://github.com/jessevig/bertviz bertviz_repo\n",
        "if not 'bertviz_repo' in sys.path:\n",
        "  sys.path += ['bertviz_repo']\n",
        "!pip install regex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puyrzyhl0bIP"
      },
      "source": [
        "We're going to clone an auxillary repository, bert-loves-chemistry, which will enable us to use the MolNet dataloader for ChemBERTa, which automatically generates scaffold splits on any MoleculeNet dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhuOe5aP0la4"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/seyonechithrananda/bert-loves-chemistry.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHMy_8vjKANZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UKF7uxYjtur"
      },
      "source": [
        "### Load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OLp-fX5W3Ah"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline, RobertaModel, RobertaTokenizer\n",
        "from bertviz import head_view\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
        "\n",
        "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru0uE-jbs8Md"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from bertviz import head_view\n",
        "\n",
        "model_version = 'seyonec/ChemBERTa-zinc-base-v1'\n",
        "model = RobertaModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfFOOp-0k4FB"
      },
      "source": [
        "### Define the Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZUS6uYck6JQ"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = '/content/drive/MyDrive/Code/train_smiles.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcsm_VY9k_BN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the data\n",
        "train = pd.read_csv(TRAIN_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgLb0WUVlBLw"
      },
      "outputs": [],
      "source": [
        "# seperate features and labels\n",
        "\n",
        "X_train = train['canonical_smiles']\n",
        "y_train = train['pIC50']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHDnUOqIlbgL"
      },
      "source": [
        "### Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppfaNRq4lfCr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoTokenizer, RobertaModel\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Define the Regression model\n",
        "class RobertaForRegression(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(RobertaForRegression, self).__init__()\n",
        "        self.roberta = model\n",
        "        # Assuming using 'roberta-base'\n",
        "        # 768 is the output dimension of the roberta model\n",
        "        # you can add more linear layers\n",
        "        # need to define the correct input dimension = output dimension of the respective above layer\n",
        "        # and required output dimension\n",
        "        self.regressor = torch.nn.Linear(768, 64)\n",
        "        self.regressor_layer_one = torch.nn.Linear(64, 32)\n",
        "        self.regressor_layer_two = torch.nn.Linear(32,1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooler_output = outputs.pooler_output\n",
        "        # make sure to defne the inputs and outputs for every layer defined in class constructor\n",
        "        regression_output = self.regressor(pooler_output)\n",
        "        regression_layer_one_output = self.regressor_layer_one(regression_output)\n",
        "        regression_layer_two_output = self.regressor_layer_two(regression_layer_one_output)\n",
        "        return regression_layer_two_output\n",
        "\n",
        "# Setup tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml0rESJcnqZ6"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdBHjfTG_dNI"
      },
      "outputs": [],
      "source": [
        "# Define loss function and learning parameters\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "learning_rate = 0.00005\n",
        "num_epochs = 50\n",
        "\n",
        "# KFold Cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "mse_scores = []\n",
        "r2_score_set = []\n",
        "\n",
        "for train_index, valid_index in kf.split(X_train):\n",
        "    X_train_fold, X_valid_fold = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
        "    y_train_fold, y_valid_fold = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
        "\n",
        "    # Create DataLoader for training fold\n",
        "    input_encodings_train = tokenizer(list(X_train_fold), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    train_dataset = TensorDataset(input_encodings_train[\"input_ids\"], input_encodings_train[\"attention_mask\"], torch.tensor(list(y_train_fold)).unsqueeze(-1))\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Create DataLoader for validation fold\n",
        "    input_encodings_valid = tokenizer(list(X_valid_fold), return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    valid_dataset = TensorDataset(input_encodings_valid[\"input_ids\"], input_encodings_valid[\"attention_mask\"], torch.tensor(list(y_valid_fold)).unsqueeze(-1))\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = RobertaForRegression(RobertaModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            batch = tuple(t.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") for t in batch)\n",
        "            input_ids_batch, attention_masks_batch, labels_batch = batch\n",
        "            outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "            loss = loss_fn(outputs, labels_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_dataloader:\n",
        "            batch = tuple(t.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") for t in batch)\n",
        "            input_ids_batch, attention_masks_batch, labels_batch = batch\n",
        "            outputs = model(input_ids_batch, attention_mask=attention_masks_batch)\n",
        "            all_preds.extend(outputs.cpu().numpy())\n",
        "            all_labels.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "    mse_scores.append(mean_squared_error(all_labels, all_preds))\n",
        "    r2_score_set.append(r2_score(all_labels, all_preds))\n",
        "\n",
        "print(\"MSE scores from 5-fold cross validation:\", mse_scores)\n",
        "print(\"R2 score : {}\".format(np.mean(r2_score_set)))\n",
        "print(\"Root Mean Square Error: \", np.sqrt(np.mean(mse_scores)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}